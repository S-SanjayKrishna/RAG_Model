# ğŸ“š Gradio RAG Chatbot with Gemma + Qdrant + PyMuPDF

A lightweight Retrieval-Augmented Generation (RAG) chatbot using a local LLM (Gemma), vector storage via Qdrant, and a Gradio interface for interaction. Users can upload PDF documents, which are then chunked, embedded, stored, and used to answer queries contextually.

---

## ğŸš€ Features

- ğŸ” Semantic search over PDF documents
- ğŸ¤– Local LLM (Gemma) for response generation
- ğŸ“¦ Vector storage using Qdrant (in-memory or remote)
- ğŸ§  Sentence Transformers for embeddings
- ğŸ“„ PDF parsing via PyMuPDF
- ğŸ›ï¸ Gradio UI for document upload and chat

---



## ğŸ—ï¸ Architecture

```
User â†” Gradio UI
        â†“
    Upload PDF
        â†“
    Parse (PyMuPDF)
        â†“
  Chunking Strategy
        â†“
Embedding (SentenceTransformers)
        â†“
Store in Qdrant (local memory or persistent)
        â†“
User Query
        â†“
  Semantic Search via Qdrant
        â†“
 Retrieve Top Chunks
        â†“
Pass context + query to Gemma LLM
        â†“
Generate Answer
        â†“
  Show Response in Chat UI
```

---

## ğŸ§© Chunking Strategy

- PDF text is split into chunks of ~300-500 characters with 50-character overlap.
- This helps maintain context continuity across sections.
- Chunking is based on paragraph structure and token count.

---

## ğŸ” Retrieval Approach

- Embeddings are generated via sentence-transformers (e.g., all-MiniLM or BGE variants).
- Top K similar vectors are retrieved from Qdrant using cosine similarity.
- Retrieved documents are concatenated into context for Gemmaâ€™s response generation.

---

## ğŸ’» Hardware Requirements

Minimum:
- CPU with AVX support
- 8 GB RAM

Recommended:
- GPU with CUDA (for fast LLM inference)
- 16 GB+ RAM

---

## ğŸ“¦ File Structure

```
â”œâ”€â”€ app.py                # Main app
â”œâ”€â”€ utils.py              # Helper functions (chunking, embedding)
â”œâ”€â”€ requirements.txt      # Optional - dependency list
â””â”€â”€ README.md             # This file
```

---

## âœ… Observations

- Using Qdrant enables fast retrieval even with large document sets.
- Chunk size significantly affects performance and relevance â€” too small loses context, too large overloads prompt.
- Gradio allows easy prototyping and deployment.

---


